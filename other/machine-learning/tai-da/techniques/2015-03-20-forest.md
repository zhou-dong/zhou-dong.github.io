---
layout: page
title: Decision Tree and Random Forest
tagline: 
categories: Machine Learning Decision tree random forest
---

Decision Tree

- 首先要考虑tree停下来的条件
- 可以用binary tree
- Build subtree
- final return
- C & RT 算法

- 避免overfitting
- Regularization by Pruning
- E in 要低，而且叶子节点也不要太多
- 可以pruning（修剪）decision tree

- 比adaboost要有效率一些
- adaboost每次都是全部切分
- C & RT在局部切分，会更有效率一些
- adaboost和decision tree的切分结果可能会不同

Random Forest

- Bagging + Decision Tree
- Bagging first, then Decision Tree

- 先对资料随机抽样，比如可以从很多个特征中，抽取一定的特征做成一棵树
- 通过上面，可以变成一个低纬度的subspace处理
- 希望做成不一样的tree，然后再把他们合起来
- 找到low-dimensional的投影平面

- out of bag estimate
- 可以用这些没有被使用的维度来判断结果好不好

- Feature Selection：怎么选择适当的特征，或者适当的输入
- 从多个维度中更有效的选取资料
    + 如，年龄和生日这两个资料重复了
    + 还有有些资料在我们的算法中是不需要的
    + 通过算法自动降维，或者自动转换

- 可以降低计算复杂度
- 可以更好得避免overfitting

- 选择本身是有计算复杂度的
- 可能会选到不好的维度，而删除了好的维度
- 如果维度选择错误的话，可能会更加的overfitting

- 相当于是一个组合爆炸的问题
- 可以给每个维度打一个分数，然后找出里面分值最高的维度
- permutation test

- 比decision tree的效果要平滑很多

Gradient Boosted Decision Tree

- adaboost + Decision Tree
