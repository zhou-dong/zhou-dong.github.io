---
layout: page
title: Review Machine Learning in Action
categories: machine-learning
---
{% include JB/setup %}

#### Machine Learning: Regression

- 使用回归模型，可以使用所有的数据来为模型服务！
- 如果只是找到最相近的几个数据点，然后来求他们的分类或者预测属性的话，那整个数据集中，只有几个点参与到的预测和分类中，其它的数据都几乎没有用到。
- 使用回归模型，不但可以用x，预测y；也可以用y，预测x

1. Simple linear regression
2. Multiple Regression
3. Assessing performance
    + Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.
4. Ridge Regression
5. Feature Selection & Lasso
6. Nearest Neighbors & Kernel Regression

---

#### 回归模型的advantage 和KNN的disadvantage

KNN的劣势：

1. 计算复杂度高，每次都要遍历整个数据集。
2. 只有最接近的K个数据参与到最后的分类或者预测中。

#### 在回归模型中，加入随着x轴的变化，周期变化的参数：

$$
y_i = w_0 + w_1t_i + w_2 \sin (2 \pi t_i / 12 - \Phi ) + \epsilon_i
$$

Equivalently

$$
y_i = w_0 + w_1t_i + w_2 \sin (2 \pi t_i / 12) + w_3 \cos (2 \pi t_i / 12) + \epsilon_i
$$

Generic basis expansion

$$
y_i = w_0h_0(x_i) + w_1h_1(x_i) + w_2 h_2(x_i)  + \cdots + w_D h_D(x_i)  + \epsilon_i \\

= \sum_{j=0}^D w_j h_j(x_i) + \epsilon_i
$$


#### 向量和矩阵 计算梯度

- Since the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:

$$
(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... + w[k]*[feature_k] - output)^2
$$

- Where we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:

$$
2*(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... + w[k]*[feature_k] - output)* [feature_i]
$$

- The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:

$$
2*error*[feature_i]
$$

- That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!
Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature[i] is just two times the dot product between the values of feature[i] and the current errors.

#### Assessing loss on the training set

- RMSE: root mean square error:

$$
\sqrt{ \frac{1}{N} \sum_{i=1}^N (y_i-f(x_i))^2 }
$$

- Generalization error vs model complexity
    + **判断结果是不是符合正态分布**
    + define overfitting
        1. training error less than other parameter
        2. test error larger than other parameter
    + split train and test data
        1. 留下足够的测试数据
        2. 然后使用剩下的所有数据做training

$$
\frac{1}{N \text{test}} \sum_{i=1}^N L(y_i-f(x_i))
$$

- 3 sources of error
    + Noise
    + Bias
    + Variance

- Bias-variance tradeoff
    + High-complexity has low bias
    + High-complexity has high variance 

- Bias of function estimator
    + 使用交叉验证
    + 在每个数据集上训练数据
    + 把每个数据集得到的结果求平均值

- Mean squre error = bias * bias + variance * variance

#### Choose model complexity

1. split data set to:
    + Training Set
    + Validation Set
    + Test Set
2. use the training set to train different polynomial degrees of function
2. use validation set to find the best model complexity (degree of polynomial)
3. use test set to test the result

#### Ridge Regression

1. Coefficients 越大，模型越fit data set.
2. If coefficients 太大的话，会overfitting
3. We expect model，fit data set， but not overfit

##### Ridge regression comes to rescue

Generally, whenever we see weights change so much in response to change in data, we believe the variance of our estimate to be large. Ridge regression aims to address this issue by penalizing "large" weights. 

##### Which means: 

- 如果一个模型overfitting的话，一般情况下coefficients会特别大
- which means we wanna model fit data set, but coefficients do not to be too large.
- We wanna balance between model fit and the magnitude of coefficients

##### Total cost = measure of fit + measure of magnitude of coefficients

- Regularization $$ \text{minimize: } RSS(w) + \lambda \| w \|^2 $$
- $$ \lambda $$ tunning parameter = balance of fit and magnitude

$$
\text{arg}_{(w, \lambda)} \min \Bigr[\sum_{i=1}^N (y_i-f_{w}(x_i))^2 + \lambda \sum_{j=i}^D w_j^2 \Bigr] 
$$

##### Cross Validation

- Leave one cross validation
- K-fold cross validation

#### Features selection

- All subsets
    1. Find feature which reduce RSS most.
    2. Fine the combination of two fearues which reduce RSS most. (Do not need to contain the step one feature).
    3. Find the combination of three features which most reduce RSS. (do not need to contain step one or step two features).
    4. repeat until convergence.

- How many choice?
    + total: $$2^D$$

- Greedy Algorithm
    1. Start at them trainning errors as all subsets(no features).
    2. Choose the one feature which reduce the RSS most.
    3. 在第一个feature的基础上，find the next feaure which reduce RSS most.
    4. repeat until convergence.

Example of All Subsets VS. Greedy Algorithm:

1. First step:
    + All subsets: sq.ft. living
    + Greedy algorithm: sq.ft. living
2. Second step:
    + All subsets: 
        - No. bedrooms
        - No. bathrooms
    + Greedy Algorithm: 
        - sq.ft. living
        - year renovated
3. Third step:
    + ...

Lasso algorithm

- Use regularization for feature selection

为什么不能在ridge regression的基础上做降维：不能直接删除weight小的feature。

- 如果几个features表示类似的属性，如：bathroom、showers，那么这两个feature的weight在ridge中，都会被降低。
- 如果同时删除几个类似的属性。那么可能会删除相关有用的信息。

##### Difference between Ridge Regression and Lasso Regression

Formula:

- Ridge regression: $$\text{arg}_w \min RSS + \lambda \| w \|_2^2$$
- Lasso Regression: $$\text{arg}_w \min RSS + \lambda \| w \|_1$$

Specific:

- Ridge regression: 
    + weight occupy bigger part in whole cost than Lasso regression
    + so every weight will shrink quickly
    + harder to find which weight is 0 when others are not.
- Lasso regression:
    + weight not occupy larger part in whole cost then Lasso regression
    + some weight will shrink quickly but some not
    + easier to sepeare 0 weight and none 0 weight 

#### Coordinate descent

Coordinate descent的思想在Lasso regression中非常重要。Often hard to find all minimum of all coordinates, but for each coordinate.

- 坐标下降法（英语：coordinate descent）是一种非梯度优化算法。算法在每次迭代中，在当前点处沿一个坐标方向进行一维搜索以求得一个函数的局部极小值。在整个过程中循环使用不同的坐标方向。
- 对于不可拆分的函数而言，算法可能无法在较小的迭代步数中求得最优解。为了加速收敛，可以采用一个适当的坐标系，例如通过主成分分析获得一个坐标间尽可能不相互关联的新坐标系
- coordinate descent不是梯度下降，它每次计算函数在一个坐标方向上的局部极小值。而不是逐步递进！
- 每次迭代，要在一个weight上做迭代，如果在这个weight上算出的梯度为0，那么说明：
    + 说明这个weight不重要
    + 已经达到了函数的极小值

$$
\hat{w}_j = \rho_j \\

\hat{w}_j = \begin{cases}
\rho_j + \lambda/2 & \text{if } \rho_j < -\lambda/2 \\
0 & \text{if } \rho_j \in [-\lambda/2, \lambda/2] \\
\rho_j - \lambda/2 & \text{if } \rho_j > \lambda / 2
\end{cases}
$$

#### Normalizing features

$$
h_j(x_k) = \frac{h_j(x_k)}{\sqrt{\sum_{i=1}^N h_j (x_i)^2 }}
$$

- Scale training columns (not rows!)
- `Apply same training scale factors in test data!`

#### Nearest Neighbors and Kernal Regression

Nearest neighbor regression

1. normalize features first
2. compute distance with query and all data set
3. find the closest data to this query.
4. predict result base on the closet data.

k-Nearest neighbors

1. find the k-nearest neighbors
2. average the neighbors price

weighted k-nearest neighbors

- different neighbors have the different distances, so should have different weights

Kernel regression

- Implement of weighted k-nearest neighbors

1. gaussian distribution to compute the weight of the neighbors
2. closest k neighbors
3. average their value

Local weighted linear regression

- use linear regression with the k neighbors

---

#### Linear classifiers

1. Score the weight of the each word (训练每个词的权重) 比如：
    + good: 1.0
    + beautiful: 1.3
    + awesome: 1.5
    + bad: -1.0
    + suck: -2.0
    + car: 0.0
    + house: 0.0
2. 把句子中的每个词的权重相加算出得分：
    + 大于0，positive
    + 小于0，negative

---

#### 基于商品的协同过滤

看来华盛顿大学的机器学习课程，才发现协同过滤真的很简单！这门课太棒了！

最简单的product-to-product推荐系统

- 首先他是一个`对角矩阵`
- 行和列一样的商品
- 这样的话，就能知道，两两商品同时出现的次数
- 这也就可以做协同过滤了

$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 20 & 1  & 2  & 100 & 15 \\
\hline
深入浅出设计模式 & 1  & 30 & 16 & 110 & 3  \\
\hline
Effective Java   & 2  & 16 & 40 & 150 &  4  \\
\hline
三体             & 100 & 110 & 150 & 10000 & 250 \\
\hline
时间简史         & 15 & 3  & 4  & 250 & 70 \\
\hline
\end{array}
$$

##### Normalizing co-occurrence matrices

Jaccard similarity，因为有些商品购买的特别多，无论买那个商品都会推荐它

who purchased `i and j` divided by who purchased `i or j`

$$
\frac{\text{# purchased } i \text{ and }  j }{\text{# purchased } i \text{ or }  j}
$$

$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 1     & 0.05  & 0.10  & 0.05 & 0.75 \\
\hline
深入浅出设计模式 & 0.05  & 1     & 0.53  & 0.27 & 0.10  \\
\hline
Effective Java   & 0.10  & 0.53  & 1     & 0.27 &0.10  \\
\hline
三体             & 0.05  & 0.27  & 0.27  & 1 & 0.25 \\
\hline
时间简史         & 0.75  & 0.10  & 0.10  & 0.25 & 1 \\
\hline
\end{array}
$$

如果用户同时购买了两件商品，或者更多的商品，可以想搜素引擎打分一样，给商品打分

1. 分别找出为这两个商品推荐的商品
2. 分别把这些权重相加，找出得分最高的商品


$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 1     & 0.05  & 0.10  & 0.05 & 0.75 \\
\hline
Effective Java   & 0.10  & 0.53  & 1     & 0.27 &0.10  \\
\hline
Result           & 1.10  & 0.58  & 1.10  & 0.32 & 0.85 \\
\hline
\end{array}
$$

所以推荐的顺序是：时间简史、深入浅出审计模式、三体

---

#### 基于内容的协同过滤

- user     有多个features
- movie  也有多个feature

- user = (f1,f2,f3,...,fn)
- movie = (k1,k2,k3,...,kn)

- user * movie = score(positve, negative)
- 用这种方法可以求出各个feature和weight
- 然后再用使用基于内容的协同过滤

##### matrix factorization

---

#### 评价系统的推荐系统

Recall

$$
\frac{\text{# liked & shown}}{\text{ # liked}}
$$

Precision

$$
\frac{\text{# liked & shown}}{\text{# shown}}
$$

Optimal recommenders

$$
Recall = 1 \\
Precision = 1
$$

Precision-recall curves

- We can use Area Under the Curve(AUC) method to evluate with curve is better.

<img width="40%" src="http://ivrl.epfl.ch/files/content/sites/ivrg/files/supplementary_material/RK_ICIP2010/images/MSSSvsOthers.jpg" alt="Precision-recall curves"/>

---

#### Word count document representation

##### Bag of words model

- Ignore order of words
- Count # of instances of each word in vocabulary

原来比较两个文章的相似度，比我想的还要简单


#### Supervised Learning method

- KNN (k nearest neighbor)
    1. Calculate the distance will all the data.
    2. Find the nearest k data
    3. From this k nearest data find the most frequent category
- Decision Tree
    1. Find the most efficient propercity to divided the datas
    2. Check if all the sub tree has been divied
    3. If not recurive steps 1 and 2
- Naive Bayes
    - 根据朴素贝叶斯概率模型来计算一个目标可能出现在哪个分类中
    - 如果出现在分来A的概率大于出现在分类B的概率，则结果为A反之为B
- Regression
    - 找出或者画出一条可以拟合数据走向的线
    - 最小二乘
    - logistic regression
    - CART Classification-and-Regression-Trees
- SVM (support vector machine)
    - 找出支撑点
    - 利用kernel把地位的数据映射到高维求解
- Adaboost
    - 可以多次重复同一函数，也可用不同函数
    - 每次计算以后，增加错误数据的权重，减小正确数据的权重

#### Unsupervised Learning method

- K means
    - 首先定义K个中心店，然后计算所有点到这些点的距离
    - 把里这些点最近的点合成一簇，
    - 找出簇内的中心店，继续迭代
- Apriori
    - 原理是认为如果一项不频繁，那些包含它的集合也不是频繁项
- FP-Growth
    - 先构建FP tree
    - 扫描FP tree，获得频繁项

#### Other optimize

- PCA
    - 找出最能表现数据的维度作为坐标轴
    - 通过这种方式来降维
- SVD
    - 把一个大的系数矩阵，切分为几个矩阵来运算
    - 通过上面的方法来达到降维的目的
