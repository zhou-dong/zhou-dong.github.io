---
layout: page
title: Support Vector Machines
tagline:
categories: machine-learning

---

$$
\min_{\theta} = C \sum_{i=1}^m \biggr[ y^{(i)} \operatorname{cost}_1 (\theta^Tx^{(i)}) + (1-y^{(i)})\operatorname{cost}_0 (\theta^Tx^{(i)}) \biggr] + \frac{1}{2} \sum_{j=1}^n \theta_j^2
$$

---

#### 要了解支撑向量机，首先要弄明白：向量相乘的几何意义。

- 向量u的模 $$\times$$ 向量v在向量u上的投影。
- $$u \times v = \| u \| \cdot \| v \| \cos(\theta)$$。

- 向量相乘，可以表示`一个向量在另一个向量方向上投影的距离`。


#### 支撑向量机的推理过程：

- 如果一个向量空间中有两个向量：一个正向量u，一个负向量v，一个分割平面c。

$$
w \cdot u \geqslant c \\
w \cdot v \leqslant c
$$

- 如果要在间隔的话，即：margin

$$
w \cdot u - b \geqslant c \\
w \cdot v + b \leqslant c
$$

- 如果把c平面看做0平面的话，即：平面上面的大于等于0，下面的小于等于0

$$
w \cdot u - b \geqslant 0 \ \ \ \text{positive} \\
w \cdot v + b \leqslant 0 \ \ \ \text{negative}
$$

- 如果是间隔为1，并且normalization公式的话：

$$
w \cdot x_+ - 1 \geqslant 0 \ \ \ \text{positive} \\
w \cdot x_- + 1 \leqslant 0 \ \ \ \text{negative} \\
\\
w \cdot x_+ \geqslant 1 \ \ \ \text{positive} \\
w \cdot x_- \leqslant -1 \ \ \ \text{negative}
$$

- 引入一个y，到上面的式子中

$$
y =
\begin{cases}
+1 \ \ \ \text{positive sample}\\
-1 \ \ \ \text{negative sample}
\end{cases}
$$

$$
w \cdot x_+ \geqslant 1 \ \ \ \text{positive} \\
w \cdot x_- \leqslant -1 \ \ \ \text{negative}
$$

$$
y_+ w \cdot x_+ \geqslant 1 \ \ \ \text{positive} \\
y_- w \cdot x_- \geqslant 1 \ \ \ \text{negative}
$$

$$
y \cdot w \cdot x \geqslant 1
$$

$$
y w x -1 \geqslant 0
$$

- 我们可以加上一个阀值：

$$
y (w x + b)  -1 \geqslant 0
$$

- `找出支撑点来`，即在支撑线上的点

$$
y (w x + b)  -1 = 0
$$

- 求支撑线之间的宽度：在+支撑线上的$$x_+$$，与在-支撑线上的$$x_-$$

$$
\text{WIDTH} =  (x_+ - x_-) \cdot \frac{w}{\| w \|}
$$

$$
x_+ = \frac{1-b}{w} \\
x_- = -\frac{1+b}{w} \\
x_+ - x_- = \frac{1-b-(-1-b)}{w} = \frac{2}{w} \\
\text{WIDTH} =  (x_+ - x_-) \cdot \frac{w}{\| w \|} = \frac{2}{w} \cdot \frac{w}{\| w \|}  = \frac{2}{\| w \|} \\
w = \operatorname{arg} \max(\frac{2}{\| w \|}) \\
= \operatorname{arg} \max(\frac{1}{\| w \|}) \\
= \operatorname{arg} \min(\| w \|) \\
= \operatorname{arg} \min(\frac{1}{2}\| w \|^2)
$$

#### 得到支撑向量机的公式表示：

$$
y =
\begin{cases}
+1 \ \ \ \text{positive sample}\\
-1 \ \ \ \text{negative sample}
\end{cases}
\ \ \ (1) \\

y (w x + b)  -1 \geqslant 0
\ \ \ (2) \\

w = \operatorname{arg} \min(\frac{1}{2}\| w \|^2)
\ \ \ (3)
$$

---

$$
\min(\frac{1}{2}\| w \|^2) \ \  \Bigl\{\text{subject to} \ \ \ y (w x + b) \geqslant 1 \Bigl\}
$$

---

- 这个其实是一个带约束的二次规划(quadratic-programming, QP)问题；
- 是一个凸问题，凸问题就是指的不会有局部最优解；
- 可以想象一个漏斗，不管我们开始的时候将一个小球放在漏斗的什么位置，这个小球最终一定可以掉出漏斗，也就是得到全局最优解。

#### 使用“拉格朗日”函数，来求解支撑向量机（Lagrange equation）

关于 Lagrange duality 简单地来说，通过给每一个约束条件加上一个 Lagrange multiplier，我们可以将它们融和到目标函数里去：

$$
\mathcal{L}(w,b,\alpha) = \frac{1}{2}\| w \|^2 - \sum \alpha_i \Bigl[y_i(wx_i + b) - 1 \Bigl] \\
\frac{\partial L}{\partial w} = w - \sum \alpha_i y_i x_i = 0\\
w = \sum_i \alpha_i y_i x_i \\
\frac{\partial L}{\partial b} = - \sum_i \alpha_i y_i = 0 \\
\sum_i \alpha_i y_i = 0
$$

$$
\mathcal{L}(w,b,\alpha) = \frac{1}{2} w^Tw - \sum_i \alpha_i \Bigl[y_i(w^Tx_i + b) - 1 \Bigl] \\
\mathcal{L}(w,b,\alpha) = \frac{1}{2} w^Tw - \sum_i \alpha_i y_i x_i w^T - \sum_i \alpha_i y_i b + \sum_i \alpha_i\\
\mathcal{L}(w,b,\alpha) = \frac{1}{2} w^Tw - ww^T - 0 + \sum_i \alpha_i\\
\mathcal{L}(w,b,\alpha) = \sum_i \alpha_i - \frac{1}{2} w^Tw \\
\mathcal{L}(w,b,\alpha) = \sum_i \alpha_i - \sum_i \sum_j \alpha_i y_i x_i^T  \alpha_j y_j x_j \\
\mathcal{L}(w,b,\alpha) = \sum_i \alpha_i - \sum_i \sum_j \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

- 新问题加上其限制条件是（对偶问题）:

$$
\max_\alpha \sum_i^n \alpha_i - \sum_i^n \sum_j^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
s.t., \alpha \geqslant 0, i=1,2,...n \\
\sum_i^n \alpha_i y_i = 0
$$


#### Kernel

- 用一个哲学例子来说：世界上本来没有两个完全一样的物体，对于所有的两个物体，我们可以通过增加维度来让他们最终有所区别，比如说两本书，从(颜色，内容)两个维度来说，可能是一样的，我们可以加上 作者 这个维度，是在不行我们还可以加入 页码，可以加入 拥有者，可以加入 购买地点，可以加入 笔记内容等等。`当维度增加到无限维的时候，一定可以让任意的两个物体可分了`。

- 如果用 $$x_1,x_2$$ 来表示二维平面的两个坐标的话，我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式：

$$
a_1x_1 + a_2x_1^2 + a_3x_2+a_4x_2^2+a_5x_1x_2+a_6 = 0
$$

- 如果我们构造另外一个五维的空间，其中五个坐标的值分别为 $$ z_1=x_1, z_2=x_1^2, z_3=x_2, z_4=x_2^2, z_5=x_1x_2 $$ 那么显然，上面的方程在新的坐标系下可以写作：

$$
a_1z_1 + a_2z_2 + a_3z_3+a_4z_4+a_5z_5+a_6 = 0 \\

\sum_{i=1}^5 a_iz_i + a_6 = 0
$$

$$
\mathcal{L}(w,b,\alpha) = \sum_i \alpha_i - \sum_i \sum_j \alpha_i \alpha_j y_i y_j z_i^T z_j
$$

- 关于新的坐标z，这正是一个 hyper plane 的方程！也就是说，如果我们做一个映射 ϕ:ℝ2→ℝ5 ，将 X 按照上面的规则映射为 Z ，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。这正是 Kernel 方法处理非线性问题的基本思想。
- 这种kernel的叫做“多项式核”。

$$
f(x) = a_1x_1 + a_2x_1^2 + a_3x_2+a_4x_2^2+a_5x_1x_2+a_6 = 0 \\
f(x) = a_1z_1 + a_2z_2 + a_3z_3+a_4z_4+a_5z_5+a_6 = 0 \\
z = \begin{bmatrix}
1 \\x_1 \\ x_1^2 \\ x_2 \\x_2^2 \\x_1x_2
\end{bmatrix}
\\
z^T = \begin{bmatrix}
1 & x_1' & x_1'^2 & x_2' & x_2'^2 &x_1'x_2'
\end{bmatrix}
\\
z^Tz = 1 + x_1'x_1 + x_1'^2x_1^2 + x_2'x_2 + x_2'^2x_2^2 + x_1'x_2'x_1x_2 \ \ \ (1)
\\
x_i = \begin{bmatrix}
x_1 \\ x_1^2
\end{bmatrix}
\\
x_j = \begin{bmatrix}
x_1' \\ x_1'^2
\end{bmatrix}
\\
(1+x^Tx')^2 = (1+x_1x_1'+x_2x_2')^2 = \\
1+ 2x_1'x_1 + x_1'^2x_1^2 + 2x_2'x_2 + x_2'^2x_2^2 + 2x_1'x_2'x_1x_2 \ \ \ (2)
$$

- 由上面的例子可以看出，把低维度空间数据映射到高维空间数据，理论上是可行的。但是如果用这种方法的话，2个维度，映射到高维以后会变成5个维度；
- 3个维度映射到高维空间以后会变成19个维度，所以维度的增加会是指数级的，如果维度变大的话，计算量会指数级的变大。
- 我们发现两个向量相加的平方与两个向量映射到高维空间的复杂度是一样的。这样我们相当于在低维运算出高维的结果。计算量会大大减小。

$$
z^Tz' = 1 + x_1'x_1 + x_1'^2x_1^2 + x_2'x_2 + x_2'^2x_2^2 + x_1'x_2'x_1x_2 \ \ \ (1) \\
(1+x^Tx')^2 = 1+ 2x_1'x_1 + x_1'^2x_1^2 + 2x_2'x_2 + x_2'^2x_2^2 + 2x_1'x_2'x_1x_2 \ \ \ (2)
$$

- 第一个是现在高维空间展开，然后再做内积
- 第二个是直接在低维空间做内积，得到的值再平方，相当于在低维运算出高维的结果。
- 随着维度的增加，第二种方法的计算量会远小于第一种。

$$
\max_\alpha \sum_i^n \alpha_i - \sum_i^n \sum_j^n \alpha_i \alpha_j y_i y_j k(x_i, x_j) \\
s.t., \alpha \geqslant 0, i=1,2,...n \\
\sum_i^n \alpha_i y_i = 0
$$

1. Polynomial Kernel
  - $$ k(x_1,x_2) = (<x_1,x_2> + R)^d$$
2. Gaussian Kernel
  - $$ k(x_1,x_2) = \text{exp} \Bigl(-\frac{\|x_1 - x_2 \|^2}{2\sigma^2} \Bigl)$$
  - 如果 σ 选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上相当于一个低维的子空间；反过来，如果 σ 选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数 σ ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。
3. Linear Kernel
  - $$ k(x_1,x_2) = <x_1,x_2> $$
  - 这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了。

*除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。*

#### Soft-margin SVM (Outliers)

$$
y_i (w^T x_i + b) \geqslant 1 \\

y_i (w^T x_i + b) \geqslant 1 - \xi_i
$$

 其中 ξi≥0 称为松弛变量 (slack variable) ，对应数据点 x 允许偏离的 functional margin 的量。当然，如果我们运行 ξi 任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 ξi 的总和也要最小：

 $$
\min(\frac{1}{2}\| w \|^2) + C\sum_{i=1}^n \xi_i
 $$

---

#### 从不同的角度解释“向量相乘”

$$
u =
\begin{bmatrix}
u_0 \\
u_1
\end{bmatrix}

\ \ \ \

v =
\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix} \\

u^Tv =
\begin{bmatrix}
u_0 & u_1
\end{bmatrix}

\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix}

= u_0 v_0 + u_1 v_1 \\
$$

<img height="200px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/2000px-Dot_Product.svg.png" />

$$
\parallel u \parallel = \sqrt{(u_0^2 + u_1^2)} \\
p = \text{length of projection of v onto u} \\
u^Tv = p \times \parallel u \parallel \\
$$

<img src="http://cdn1.askiitians.com/Images/2014108-144310695-8606-dot-product-image.PNG" />


$$
u \times v = \parallel u \parallel \parallel v \parallel \cos(\theta)
$$

---

#### Reference：

sth

https://www.youtube.com/watch?v=_PwhiWxHK8o

http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html

http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982684.html

http://blog.pluskid.org/?page_id=683
